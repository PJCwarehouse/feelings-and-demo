一.使用爬虫爬取知乎日报的内容
所需知识：
1.爬虫是通过scrapy模块自带的cmdline.execute来启动的
2.将自动执行脚本做到scrapy爬虫的外部，使爬虫每天循环的执行一次
二.将爬虫爬取的数据存储到mongodb和mysql两个数据库中
所需知识：
1.创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。
2.集合中插入文档使用 insert_one() 方法
三.将每天爬取的新数据通过脚本发送邮件到指定邮箱
所需知识：
1.首先需要设置QQ邮箱POP3/SMTP服务
2.用到的库是 smtplib 和 email，将爬取的数据保存在附件中，发送到指定邮箱